{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Workspace/bookstore/\" # insert the absolute path to your folder\n",
    "\n",
    "\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "lines_dict = {}\n",
    "\n",
    "for file in filenames:\n",
    "    filepath = path+file\n",
    "    lines_dict[file] = []\n",
    "    # start reading the decompressed file's lines one by one\n",
    "    # gzip takes care of the decompression\n",
    "    for line in gzip.open(filepath, 'r'):\n",
    "        # append each line separately into the lines list\n",
    "        # this is done because json.loads can't process more\n",
    "        # than 1 record at a time, see here for more:\n",
    "        # https://stackoverflow.com/questions/48140858/json-decoder-jsondecodeerror-extra-data-line-2-column-1-char-190\n",
    "        lines_dict[file].append(json.loads(line.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point we have a lines_dict object, which has 3 keys: 1 for each loaded file\n",
    "# You can view the keys as follows:\n",
    "print(lines_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each key, the dictionary produces a list of json rows.\n",
    "# Each json row corresponds to one line for the selected file.\n",
    "# The code below simply prints the number of rows (i.e. number of json lines)\n",
    "# for each dictionary key\n",
    "#for idx, value in enumerate(lines_dict.values()):\n",
    "    #print(f\"The No. {idx+1} dictionary entry corresponds to a file with {len(value)} json rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data for the books\n",
    "isbns, titles, publishers, publication_year, book_id, authors = [], [], [], [], [], []\n",
    "for line in lines_dict['goodreads_books_children.json.gz']:\n",
    "    isbns.append(line['isbn'])\n",
    "    titles.append(line['title'])\n",
    "    publishers.append(line['publisher'])\n",
    "    publication_year.append(line['publication_year'])\n",
    "    book_id.append(line['book_id'])\n",
    "    authors.append(line['authors'])\n",
    "    \n",
    "\n",
    "# keep only the first 100 rows for isbns that are not null\n",
    "result = pd.DataFrame({'isbn' : isbns, 'book_id': book_id, 'title' : titles, 'authors': authors, 'publisher': publishers, 'publication_year': publication_year})\n",
    "result[~result['isbn'].str.contains('')]\n",
    "result.replace('', np.nan, inplace = True)\n",
    "df_1 = result.dropna()\n",
    "df1 = df_1.loc[0:150]\n",
    "df = df1.drop([42,46,97,104,135]) # extract data with non-english characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the data that will populate the tables BOOKS_18 & PUBLISHERS_18\n",
    "books_id = df[\"book_id\"].tolist()\n",
    "#print(books_id)\n",
    "books_isbn = df[\"isbn\"].tolist()\n",
    "#print(books_isbn)\n",
    "books_title = df[\"title\"].tolist()\n",
    "\n",
    "publisher = df[\"publisher\"].tolist()\n",
    "\n",
    "# iterate over each string in the publishers list replacing any single quote characters with two single quote characters\n",
    "publisher = [name.replace(\"\\'\", \"\\'\\'\" ) for name in publisher]\n",
    "\n",
    "publication_year= df[\"publication_year\"].tolist()\n",
    "\n",
    "# select publisher column, turn to df and drop the duplicates\n",
    "publisher_column = df['publisher']\n",
    "publisher2 = pd.DataFrame(publisher_column)\n",
    "publ = publisher2.drop_duplicates()\n",
    "\n",
    "publ2 = publ[\"publisher\"].tolist() #turn back to list\n",
    "\n",
    "# iterate over each string in the publishers list replacing any single quote characters with two single quote characters\n",
    "publ2  = [publisher.replace(\"\\'\", \"\\'\\'\" ) for publisher in publ2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and store data from the authors column. The data resides in lists of dictionairies\n",
    "authors_list = df[\"authors\"].tolist()\n",
    "\n",
    "\n",
    "author_id_values = []\n",
    "role_values = []\n",
    "\n",
    "# iterate over the sub-lists in the list\n",
    "for sub_list in authors_list:\n",
    "  # extract the dictionary from the sub-list\n",
    " for d in sub_list:\n",
    " \n",
    "  \n",
    "  # extract the values of the \"author_id\" and \"role\" keys\n",
    "  author_id_value = d[\"author_id\"]\n",
    "  role_value = d[\"role\"]\n",
    "  \n",
    "  # append the values to the lists\n",
    "  author_id_values.append(author_id_value)\n",
    "  role_values.append(role_value)\n",
    "\n",
    "# store id's and roles as separate dataframe\n",
    "  df_3= pd.DataFrame({'author_id':author_id_values, 'role':role_values})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data for the authors\n",
    "author_id, name = [], []\n",
    "for line in lines_dict['goodreads_book_authors.json.gz']:\n",
    "    author_id.append(line['author_id'])\n",
    "    name.append(line['name'])\n",
    "    \n",
    "    \n",
    "\n",
    "result_2 = pd.DataFrame({'author_id' : author_id, 'name' : name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the id's of relevant authors that contributed in the fist 100 books\n",
    "df_2= result_2.loc[result_2['author_id'].isin(author_id_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a dataframe to store the data that will populate the table AUTHORS_18\n",
    "df_merged = df_2.merge(df_3, on= 'author_id')\n",
    "df_authors = df_merged.drop_duplicates() #the merge.() method creates duplicates so we need to drop them\n",
    "\n",
    "# store each column as a list of strings\n",
    "author_id = df_authors[\"author_id\"].tolist()\n",
    "author_name = df_authors[\"name\"].tolist()\n",
    "role = df_authors[\"role\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data to populate the REVIEWS_18 table\n",
    "books, ratings, review_id, timestamp, nickname, review_txt = [], [], [], [], [], []\n",
    "for line in lines_dict['goodreads_reviews_children.json.gz']:\n",
    "    ratings.append(line['rating'])\n",
    "    books.append(line['book_id'])\n",
    "    review_id.append(line['review_id'])\n",
    "    timestamp.append(line['date_added'])\n",
    "    nickname.append(line['user_id'])\n",
    "    review_txt.append(line['review_text'])\n",
    "    \n",
    "\n",
    "\n",
    "result = pd.DataFrame({'book_id' : books, 'ratings' : ratings, 'review_id': review_id, 'timestamp': timestamp, 'nickname': nickname, 'review': review_txt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the relevant reviews for our 100 books\n",
    "df_4= result.loc[result['book_id'].isin(books_id)]\n",
    "\n",
    "# create a dataframe with books' id's and isbns only\n",
    "books_id_df = pd.DataFrame({'book_id':books_id, 'isbn':books_isbn})\n",
    "\n",
    "# merge the dataframes to create the final edition of the reviews dataframe\n",
    "df_5 = df_4.merge(books_id_df, on = 'book_id')\n",
    "df_reviews = df_5.drop_duplicates() #just in case there exist duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the data that will populate REVIEWS_18 table\n",
    "rev_id = df_reviews[\"review_id\"].tolist()\n",
    "rev_isbn = df_reviews[\"isbn\"].tolist()\n",
    "nickname = df_reviews[\"nickname\"].tolist()\n",
    "rev_rating = df_reviews[\"ratings\"].tolist()\n",
    "rev_timestamp = df_reviews[\"timestamp\"].tolist()\n",
    "rev_text = df_reviews[\"review\"].tolist()\n",
    "rev_text  = [review.replace(\"\\'\", \"\\'\\'\" ) for review in rev_text] # use the .replace method to escape the apostroph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write SQL statements for populating the relations\n",
    "with open (\"data-18.sql\", 'w') as f:\n",
    "    f.write(\"INSERT INTO BOOKS_18(isbn,title,publisher,publication_year)\\nVALUES\\n\")\n",
    "    for idx, (isbn, title, publisher, publication_year) in enumerate (zip(books_isbn, books_title, publisher, publication_year)):\n",
    "        if idx !=len(books_isbn)-1:\n",
    "            f.write(f\"{(isbn, title, publisher, publication_year)},\\n\")\n",
    "        else:\n",
    "            f.write(f\"{(isbn, title, publisher, publication_year)};\")\n",
    "    f.write(\"INSERT INTO PUBLISHERS_18(publisher)\\nVALUES\\n\")\n",
    "    for idx, (publisher) in enumerate (publ2):\n",
    "        if idx !=len(publ2)-1:\n",
    "            f.write(f\"{(publisher)},\\n\")\n",
    "        else:\n",
    "            f.write(f\"{(publisher)};\")\n",
    "    f.write(\"INSERT INTO AUTHORS_18(id, name, role)\\nVALUES\\n\")\n",
    "    for idx, (id, name, role) in enumerate (zip(author_id, author_name, role)):\n",
    "        if idx !=len(author_id)-1:\n",
    "            f.write(f\"{(id, name, role)},\\n\")\n",
    "        else:\n",
    "            f.write(f\"{(id, name, role)};\")\n",
    "    f.write(\"INSERT INTO BOOKS_AUTHORS_18(isbn, id)\\nVALUES\\n\")\n",
    "    for idx, (isbn, id) in enumerate (zip(books_isbn, author_id_values)):\n",
    "        if idx !=len(books_isbn)-1:\n",
    "            f.write(f\"{(isbn, id)},\\n\")\n",
    "        else:\n",
    "            f.write(f\"{(isbn, id)};\")\n",
    "    f.write(\"INSERT INTO REVIEWS_18(review_id, isbn, nickname, ratings, timestamp, review)\\nVALUES\\n\")\n",
    "    for idx, (review_id, isbn, nickname, ratings, timestamp, review) in enumerate (zip(rev_id, rev_isbn, nickname, rev_rating, rev_timestamp, rev_text)):\n",
    "        if idx !=len(rev_id)-1:\n",
    "            f.write(f\"{(review_id, isbn, nickname, ratings, timestamp, review)},\\n\")\n",
    "        else:\n",
    "            f.write(f\"{(review_id, isbn, nickname, ratings, timestamp, review)};\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08f69f058332a53b988b03342f3698f4bbcedd00667755503e522d87b70d0004"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
